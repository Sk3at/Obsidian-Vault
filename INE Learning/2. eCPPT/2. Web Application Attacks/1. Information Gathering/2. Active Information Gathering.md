Crawling & Spidering
El crawler, también conocido como araña de la web, es un software o webbot que se encarga de recorrer los enlaces de las páginas webs de una forma automática y sistemática.
Normalmente, un crawler dispone de un conjunto inicial de URLs, conocidas como semillas, y va descargando las páginas Web asociadas a las semillas y buscando dentro de éstas otras URLs.
Cada nueva URL encontrada se añade a la lista de URLs que la araña Web debe visitar. Es decir, recoleta URL’s para posteriormente procesarlas. Así, el motor de búsqueda creará un índice de las páginas descargadas para proporcionar búsquedas más rápidas.
El crawling es mas bien un procedimiento de obtencion de informacion PASIVO ya que mapea la web publicamente disponible. La herramienta que podemos utilizar es el passive crawler de burp suite para mapear una aplicacion web y asi entender mejor como esta compuesta y su funcionamiento.

