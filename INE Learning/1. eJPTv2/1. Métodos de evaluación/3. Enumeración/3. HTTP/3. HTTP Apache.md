Ejercicio:
1. Encontrar el tipo de webserver y la versión con nmap.
2. Encontrar el tipo de webserver y la versión con metasploit.
3. Chequear la aplicación web con curl.
4. Chequear la aplicación web con wget.
5. Chequear la aplicación web con lynx.
6. Realizar un ataque de fuerza bruta para descubrir directorios con brute_dirs en metasploit.
7. Usar dirb con el diccionario /usr/share/metasploit-framework/data/wordlists/directory.txt
8. Verificar el archivo robots.txt

![[../../../Images/Pasted image 20230929110140.png]]


Solución:

Realizamos ping a la maquina objetivo para verificar si responde al protocolo ICMP.
![[../../../Images/Pasted image 20230929110353.png]]
Realizamos un escaneo con nmap y la opción -sV para descubrir la versión de los servicios en ejecución.
![[../../../Images/Pasted image 20230929110514.png]]
Vemos que en el puerto 80 se encuentra corriendo un webserver de apache 2.4.18
Ahora abrimos msfconsole para encontrar el tipo de web y la versión usando metasploit.
![[../../../Images/Pasted image 20230929111108.png]]
Aprovechando que estamos dentro de metasploit configuramos y ejecutamos el brute_dirs para enumerar directorios:
![[../../../Images/Pasted image 20230929111324.png]]
Se encontraron 2 directorios a los que podriamos acceder (200).
Ahora buscamos dentro de metasploit si hay alguna forma de obtener el archivo robots.txt
Vemos que existe el modulo que necesitamos:
![[../../../Images/Pasted image 20230929111439.png]]
Ahora procedemos a configurarlo y ejecutarlo.
![[../../../Images/Pasted image 20230929111556.png]]
Saliendo de msfconsole intentaremos obtener el robots.txt con algún script que cumpla esa funcion en nmap.
![[../../../Images/Pasted image 20230929111751.png]]
Ahora procedemos a realizar la búsqueda de directorios de la aplicación web con dirb y el diccionario que se nos indico.
![[../../../Images/Pasted image 20230929111946.png]]


Ahora procedemos a utilizar las distintas herramientas para ver el contenido de la aplicación web en nuestra terminal.
Comenzamos con curl:
![[../../../Images/Pasted image 20230929112123.png]]
Seguimos con lynx:
![[../../../Images/Pasted image 20230929112152.png]]
Por ultimo utilizamos wget:
![[../../../Images/Pasted image 20230929112226.png]]
Wget descargo la pagina index en nuestra maquina, la cual ahora podemos abrir con cualquier comando para visualizarla.


Como nota adicional, si le prestamos atención al robots obtenido vemos que hay un directorio /cgi-bin/ deshabilitado para el agente \*  así que podriamos intentar ingresar a el como el otro agente disponible en robots utilizando curl:
![[../../../Images/Pasted image 20230929112542.png]]
Vemos que tambien se encuentra prohibido.

Una funcionalidad de curl es poder especificar el "agente" que requiere la página.


![[../../../Images/Pasted image 20230929112659.png]]
